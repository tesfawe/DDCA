{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f93e1633-3453-408d-a5a6-d3ccb06fcf22",
   "metadata": {},
   "source": [
    "Name: Tesfay Tesfay \\\n",
    "Email: tesfay.tesfay@abo.fi \\\n",
    "S_ID: 2402263"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62ebb5b-4d1a-49f4-98a9-ea0c35b5ea36",
   "metadata": {},
   "source": [
    "# Exercise 1: Introduction to Delta Lake with PySpark\n",
    "\n",
    "\n",
    "This exercise demonstrates the basic functionalities of Delta Lake using PySpark. We'll work with a dataset on New York air quality (`air_quality_data.csv`) to showcase the following operations:\n",
    "\n",
    "1. Reading and Writing Delta Tables\n",
    "2. Update\n",
    "3. Append\n",
    "4. Delete\n",
    "5. Time Travel\n",
    "6. Vacuuming (Cleanup)\n",
    "\n",
    "\n",
    "Helpful links:\n",
    "\n",
    "https://docs.delta.io/latest/quick-start.html#read-data&language-python\n",
    "\n",
    "https://docs.delta.io/latest/index.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1dde99b-fd0e-48f6-8813-55a0321576cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: delta-spark==3.0.0 in /opt/conda/lib/python3.11/site-packages (3.0.0)\n",
      "Requirement already satisfied: pyspark<3.6.0,>=3.5.0 in /usr/local/spark/python (from delta-spark==3.0.0) (3.5.1)\n",
      "Requirement already satisfied: importlib-metadata>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from delta-spark==3.0.0) (7.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.11/site-packages (from importlib-metadata>=1.0.0->delta-spark==3.0.0) (3.17.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.11/site-packages (from pyspark<3.6.0,>=3.5.0->delta-spark==3.0.0) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "!pip install delta-spark==3.0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95822a50-03e5-49aa-8568-f64c0f411b00",
   "metadata": {},
   "source": [
    "## Step 1: Initializing PySpark and Delta Lake Environment\n",
    "\n",
    "We'll configure the Spark session with Delta Lake support.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4fd06817-8ac5-4020-9a1e-51b2c7ea9b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session with Delta Lake configured successfully!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://np-deep-lilac-tecoma-578488897c-z862d:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Exercise1</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7eff33fb6890>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from delta import configure_spark_with_delta_pip\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Configure the Spark session with Delta support\n",
    "builder = SparkSession.builder \\\n",
    "    .appName(\"Exercise1\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:3.0.0\")\n",
    "\n",
    "# Create the Spark session\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "print(\"Spark session with Delta Lake configured successfully!\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b1444f-b76c-4bc8-910a-1d88087a84c1",
   "metadata": {},
   "source": [
    "\n",
    "**Question:** \n",
    "Why are we using `configure_spark_with_delta_pip` to configure Spark instead of just running it as is? (1p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b2572c-8833-42be-b133-4606b208e119",
   "metadata": {},
   "source": [
    "**Ans:** To insure that the Spark session is correctly configured with Delta Lake-specific configurations which are not included by default. This allows Spark to handle Delta Lake operations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe097ef-2c81-42d6-9edd-75b3c8915b7d",
   "metadata": {},
   "source": [
    "## Step 2: Loading Air Quality Data (1p)\n",
    "\n",
    "We'll load the air quality dataset (`air_quality_data.csv`) and inspect its structure. After that, we save it as a Spark DataFrame. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c3789f1-0a83-4786-932a-f14b6f2cc8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+-------+-------------+--------------+--------------+----------+----------+--------------------+\n",
      "|Unique_ID|             Name|Measure|Geo_Type_Name|Geo_Place_Name|   Time_Period|Start_Date|Data_Value|Air_Quality_Category|\n",
      "+---------+-----------------+-------+-------------+--------------+--------------+----------+----------+--------------------+\n",
      "|   179772|        Emissions|Density|        UHF42|        Queens|         Other|    1/1/15|       0.3|                Good|\n",
      "|   179785|        Emissions|Density|        UHF42|       Unknown|         Other|    1/1/15|       1.2|                Good|\n",
      "|   178540|General Pollution|  Miles|        UHF42|       Unknown|Annual Average|   12/1/11|       8.6|                Good|\n",
      "|   178561|General Pollution|  Miles|        UHF42|        Queens|Annual Average|   12/1/11|         8|                Good|\n",
      "|   823217|General Pollution|  Miles|        UHF42|        Queens|        Summer|    6/1/22|       6.1|                Good|\n",
      "|   177910|General Pollution|  Miles|        UHF42|       Unknown|        Summer|    6/1/12|        10|                Good|\n",
      "|   177952|General Pollution|  Miles|        UHF42|       Unknown|        Summer|    6/1/13|       9.8|                Good|\n",
      "|   177973|General Pollution|  Miles|        UHF42|        Queens|        Summer|    6/1/13|       9.8|                Good|\n",
      "|   177931|General Pollution|  Miles|        UHF42|        Queens|        Summer|    6/1/12|       9.6|                Good|\n",
      "|   742274|General Pollution|  Miles|        UHF42|        Queens|        Summer|    6/1/21|       7.2|                Good|\n",
      "|   178582|General Pollution|  Miles|        UHF42|       Unknown|Annual Average|   12/1/12|       8.2|                Good|\n",
      "|   178583|General Pollution|  Miles|        UHF42|       Unknown|Annual Average|   12/1/12|       8.1|                Good|\n",
      "|   547477|General Pollution|  Miles|        UHF42|        Queens|Annual Average|    1/1/17|       6.8|                Good|\n",
      "|   547417|General Pollution|  Miles|        UHF42|       Unknown|Annual Average|    1/1/17|       6.8|                Good|\n",
      "|   177784|General Pollution|  Miles|        UHF42|       Unknown|        Summer|    6/1/09|      10.6|            Moderate|\n",
      "|   547414|General Pollution|  Miles|        UHF42|       Unknown|Annual Average|    1/1/17|       7.1|                Good|\n",
      "|   130413|        Emissions|Density|        UHF42|       Unknown|         Other|    1/1/13|       0.9|                Good|\n",
      "|   130412|        Emissions|Density|        UHF42|       Unknown|         Other|    1/1/13|       1.7|                Good|\n",
      "|   130434|        Emissions|Density|        UHF42|        Queens|         Other|    1/1/13|         0|                Good|\n",
      "|   410847|General Pollution|  Miles|        UHF42|        Queens|        Summer|    6/1/16|       6.9|                Good|\n",
      "+---------+-----------------+-------+-------------+--------------+--------------+----------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Shape: 18016 x 9\n"
     ]
    }
   ],
   "source": [
    "# Load CSV data\n",
    "csv_path = \"../shared/air_quality_data.csv\"\n",
    "df = spark.read.csv(csv_path, header=True)\n",
    "\n",
    "# Display the data \n",
    "df.show()\n",
    "\n",
    "# Get the number of rows\n",
    "row_count = df.count()\n",
    "\n",
    "# Get the number of columns\n",
    "column_count = len(df.columns)\n",
    "print(f\"Shape: {row_count} x {column_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30e791e-0989-4522-9d12-e7a47f85309d",
   "metadata": {},
   "source": [
    "## Step 3: Writing Data to Delta Format (1p)\n",
    "\n",
    "We will save the dataset as a Delta table for further operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80018bcd-b350-432a-8432-a93e2869731a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to Delta format at delta-table-v-02\n"
     ]
    }
   ],
   "source": [
    "# Save DataFrame to Delta format\n",
    "delta_path = \"delta-table-v-02\"\n",
    "\n",
    "df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(delta_path)\n",
    "\n",
    "print(f\"Data saved to Delta format at {delta_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0aa23f-a251-47e0-b53f-b6f39b887490",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Delta Lake Operations: Update, Append, Delete, and More (16p)\n",
    "\n",
    "Now that we have saved our data as a delta table, let's run some basic operations on it.\n",
    "\n",
    "- **Update**: Modifying rows based on conditions.\n",
    "- **Append with Schema Evolution**: Adding new data while evolving the schema.\n",
    "- **Delete**: Removing rows based on conditions.\n",
    "- **Time Travel**: Querying historical versions of the table.\n",
    "- **Vacuum**: Cleaning up unreferenced files to optimize storage.\n",
    "\n",
    "We’ll use a Delta table at `delta_path` to showcase these features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba1ad8b-ab70-41bc-abc3-93486dffbb70",
   "metadata": {},
   "source": [
    "## 1. Update Rows in the Delta Table (2p)\n",
    "\n",
    "This operation demonstrates how to update specific rows in the Delta table. \n",
    "In this case, we replace the value `'Unknown'` in the `Geo_Place_Name` column with `'Not_Specified'`. (2p)\n",
    "\n",
    "**Code:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0b0d000-c19c-4285-b1d8-adae96262f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update completed!\n",
      "+--------------+-----+\n",
      "|Geo_Place_Name|count|\n",
      "+--------------+-----+\n",
      "|        Queens| 1466|\n",
      "|      Brooklyn|  280|\n",
      "| Staten Island|  368|\n",
      "| Not_Specified|14546|\n",
      "|     Manhattan|  439|\n",
      "|         Bronx|  917|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Load Delta Table\n",
    "delta_table = DeltaTable.forPath(spark, delta_path)\n",
    "\n",
    "# Update operation: Update rows where Geo_Place_Name is 'Unknown'\n",
    "delta_table.update(\n",
    "    condition=\"Geo_Place_Name = 'Unknown'\",\n",
    "    set={\"Geo_Place_Name\": \"'Not_Specified'\"}\n",
    ")\n",
    "\n",
    "print(\"Update completed!\")\n",
    "\n",
    "# Create a temporary view to query the Delta table\n",
    "delta_table.toDF().createOrReplaceTempView(\"delta_table_view\")\n",
    "\n",
    "# Use spark.sql to visualize the changes\n",
    "spark.sql(\"\"\"\n",
    "    SELECT Geo_Place_Name, COUNT(*) AS count\n",
    "    FROM delta_table_view\n",
    "    GROUP BY Geo_Place_Name\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabd8dec-6e2e-48b5-bc3a-a3eeb2134935",
   "metadata": {},
   "source": [
    "**Question:**  \n",
    "What happens when we update rows in a Delta table? How does Delta handle changes differently compared to a standard data format? (1p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c84701-62b7-4a79-89f3-e15e8d96769a",
   "metadata": {},
   "source": [
    "**Ans:** Delta Lake saves the changes as a new version of the data instead of overwriting the existing data. This ensures ACID properties compliance and maintains a transaction log. Compared to a standard data format, Delta Lake enables features like time travel, data lineage, and recovery, which are not available in traditional data formats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bc191b-5a19-49f3-b444-9cebe92567e3",
   "metadata": {},
   "source": [
    "## 2. Append Data with Schema Evolution (2p)\n",
    "\n",
    "Here, we demonstrate appending new rows to the Delta table. Additionally, we include a new column, \n",
    "`Source`, to showcase Delta Lake’s schema evolution capabilities.\n",
    "\n",
    "**Steps:**\n",
    "1. Create a new DataFrame with an additional column (`Source`).\n",
    "2. Use `mergeSchema=True` to allow schema evolution.\n",
    "3. Append the new data to the Delta table.\n",
    "4. Query the table using spark.sql to visualize changes\n",
    "\n",
    "**Code:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e1b6b66-a605-4fbe-bbb5-f1a3fdca47cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Append with schema evolution completed!\n",
      "+---------+-----------------+-------+-------------+--------------+--------------+----------+----------+--------------------+------+\n",
      "|Unique_ID|             Name|Measure|Geo_Type_Name|Geo_Place_Name|   Time_Period|Start_Date|Data_Value|Air_Quality_Category|Source|\n",
      "+---------+-----------------+-------+-------------+--------------+--------------+----------+----------+--------------------+------+\n",
      "|   179772|        Emissions|Density|        UHF42|        Queens|         Other|    1/1/15|       0.3|                Good|  NULL|\n",
      "|   179785|        Emissions|Density|        UHF42| Not_Specified|         Other|    1/1/15|       1.2|                Good|  NULL|\n",
      "|   178540|General Pollution|  Miles|        UHF42| Not_Specified|Annual Average|   12/1/11|       8.6|                Good|  NULL|\n",
      "|   178561|General Pollution|  Miles|        UHF42|        Queens|Annual Average|   12/1/11|         8|                Good|  NULL|\n",
      "|   823217|General Pollution|  Miles|        UHF42|        Queens|        Summer|    6/1/22|       6.1|                Good|  NULL|\n",
      "|   177910|General Pollution|  Miles|        UHF42| Not_Specified|        Summer|    6/1/12|        10|                Good|  NULL|\n",
      "|   177952|General Pollution|  Miles|        UHF42| Not_Specified|        Summer|    6/1/13|       9.8|                Good|  NULL|\n",
      "|   177973|General Pollution|  Miles|        UHF42|        Queens|        Summer|    6/1/13|       9.8|                Good|  NULL|\n",
      "|   177931|General Pollution|  Miles|        UHF42|        Queens|        Summer|    6/1/12|       9.6|                Good|  NULL|\n",
      "|   742274|General Pollution|  Miles|        UHF42|        Queens|        Summer|    6/1/21|       7.2|                Good|  NULL|\n",
      "|   178582|General Pollution|  Miles|        UHF42| Not_Specified|Annual Average|   12/1/12|       8.2|                Good|  NULL|\n",
      "|   178583|General Pollution|  Miles|        UHF42| Not_Specified|Annual Average|   12/1/12|       8.1|                Good|  NULL|\n",
      "|   547477|General Pollution|  Miles|        UHF42|        Queens|Annual Average|    1/1/17|       6.8|                Good|  NULL|\n",
      "|   547417|General Pollution|  Miles|        UHF42| Not_Specified|Annual Average|    1/1/17|       6.8|                Good|  NULL|\n",
      "|   177784|General Pollution|  Miles|        UHF42| Not_Specified|        Summer|    6/1/09|      10.6|            Moderate|  NULL|\n",
      "|   547414|General Pollution|  Miles|        UHF42| Not_Specified|Annual Average|    1/1/17|       7.1|                Good|  NULL|\n",
      "|   130413|        Emissions|Density|        UHF42| Not_Specified|         Other|    1/1/13|       0.9|                Good|  NULL|\n",
      "|   130412|        Emissions|Density|        UHF42| Not_Specified|         Other|    1/1/13|       1.7|                Good|  NULL|\n",
      "|   130434|        Emissions|Density|        UHF42|        Queens|         Other|    1/1/13|         0|                Good|  NULL|\n",
      "|   410847|General Pollution|  Miles|        UHF42|        Queens|        Summer|    6/1/16|       6.9|                Good|  NULL|\n",
      "+---------+-----------------+-------+-------------+--------------+--------------+----------+----------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Shape: 18018 x 10\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Create new data directly\n",
    "# Values of column 1 and 8 are changed to String\n",
    "new_data = [\n",
    "    (179808, \"Emissions\", \"Density\", \"UHF42\", \"Queens\", \"Other\", \"2015-01-05\", 0.7, \"Good\", \"SensorA\"),\n",
    "    (179809, \"Emissions\", \"Density\", \"UHF42\", \"Bronx\", \"Other\", \"2015-01-05\", 1.4, \"Moderate\", \"SensorB\")\n",
    "]\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "new_data_df = spark.createDataFrame(new_data, [\n",
    "    \"Unique_ID\", \"Name\", \"Measure\", \"Geo_Type_Name\", \"Geo_Place_Name\", \n",
    "    \"Time_Period\", \"Start_Date\", \"Data_Value\", \"Air_Quality_Category\", \"Source\"\n",
    "])\n",
    "\n",
    "# Due to data type incompatibility the following two column data types are changed to string to match existing Delta table schema\n",
    "new_data_df = new_data_df.withColumn(\"Unique_ID\", col(\"Unique_ID\").cast(\"string\"))\n",
    "\n",
    "new_data_df = new_data_df.withColumn(\"Data_Value\", col(\"Data_Value\").cast(\"string\"))\n",
    "\n",
    "# Append new data with schema evolution\n",
    "new_data_df.write.format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .save(delta_path)\n",
    "\n",
    "print(\"Append with schema evolution completed!\")\n",
    "\n",
    "# Load the Delta table\n",
    "delta_table = DeltaTable.forPath(spark, delta_path)\n",
    "\n",
    "# Create a temporary view for querying\n",
    "delta_table.toDF().createOrReplaceTempView(\"delta_table_view\")\n",
    "\n",
    "# Use spark.sql to visualize the updates\n",
    "spark.sql(\"SELECT * FROM delta_table_view\").show()\n",
    "\n",
    "# Check if the dimention has been changed \n",
    "\n",
    "# Get the number of rows\n",
    "row_count = delta_table.toDF().count()\n",
    "\n",
    "# Get the number of columns\n",
    "column_count = len(delta_table.toDF().columns)\n",
    "\n",
    "print(f\"Shape: {row_count} x {column_count}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125db97d-bf25-4a6d-aab9-c7cd54ccc14b",
   "metadata": {},
   "source": [
    "**Question:**  \n",
    "When appending new data to a Delta table, what benefits does Delta provide compared to other data formats? (1p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2fa278-7eb2-4ebf-8756-1673abba0571",
   "metadata": {},
   "source": [
    "**Ans:** Delta Lake supports schema evolution and ensures ACID transactions during appends. This minimizes the risk of data corruption and allows for the seamless integration of new columns or data types, making it easier to adapt to evolving data schemas without disrupting existing data pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f404c799-5ee5-4819-8384-1f1e7da7e2c8",
   "metadata": {},
   "source": [
    "## 3. Delete Rows from the Delta Table (2p)\n",
    "\n",
    "This operation removes rows from the Delta table based on a condition. \n",
    "Here, we delete rows where the `Geo_Place_Name` column has the value `'Not_Specified'`.\n",
    "\n",
    "**Code:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "379c5c67-df0e-44c2-9945-f69d681d6406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with Geo_Place_Name = 'Not_Specified' have been deleted!\n",
      "+--------------+-----+\n",
      "|Geo_Place_Name|count|\n",
      "+--------------+-----+\n",
      "|        Queens| 1467|\n",
      "|      Brooklyn|  280|\n",
      "| Staten Island|  368|\n",
      "|     Manhattan|  439|\n",
      "|         Bronx|  918|\n",
      "+--------------+-----+\n",
      "\n",
      "Shape: 3472 x 10\n"
     ]
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Load the Delta table\n",
    "delta_table = DeltaTable.forPath(spark, delta_path)\n",
    "\n",
    "# Delete rows where Geo_Place_Name is 'Not_Specified'\n",
    "delta_table.delete(\"Geo_Place_Name = 'Not_Specified'\")\n",
    "\n",
    "print(\"Rows with Geo_Place_Name = 'Not_Specified' have been deleted!\")\n",
    "\n",
    "# Create a temporary view to query the Delta table\n",
    "delta_table.toDF().createOrReplaceTempView(\"delta_table_view\")\n",
    "\n",
    "# Query to visualize the changes\n",
    "spark.sql(\"\"\"\n",
    "    SELECT Geo_Place_Name, COUNT(*) AS count\n",
    "    FROM delta_table_view\n",
    "    GROUP BY Geo_Place_Name\n",
    "\"\"\").show()\n",
    "\n",
    "# Get the number of rows\n",
    "row_count = delta_table.toDF().count()\n",
    "\n",
    "# Get the number of columns\n",
    "column_count = len(delta_table.toDF().columns)\n",
    "print(f\"Shape: {row_count} x {column_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8347cd01-615f-4126-b8aa-d01788007a9e",
   "metadata": {},
   "source": [
    "**Question:**  \n",
    "What if we accidentally delete rows in a Delta table? Can we recover them? (1p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b891b7-f9d3-4ba9-98f5-43f16c3f1138",
   "metadata": {},
   "source": [
    "**Ans:** Yes, Delta Lake's time travel feature allows us to query historical versions of the table and recover accidentally deleted rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3deb6bc7-819b-4d28-be25-c5a87fb32ae1",
   "metadata": {},
   "source": [
    "## 4. Time Travel: Query a Previous Version (2p)\n",
    "\n",
    "Delta Lake allows you to query historical versions of the table using the `versionAsOf` option. Visualize the previous versions of the table and query one of the historical versions.\n",
    "\n",
    "**Code:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50ffb9fa-6e33-4a29-bb24-78be6ffe6325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table History:\n",
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|version|           timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|      3|2025-01-21 16:40:...|  NULL|    NULL|   DELETE|{predicate -> [\"(...|NULL|    NULL|     NULL|          2|  Serializable|        false|{numRemovedFiles ...|        NULL|Apache-Spark/3.5....|\n",
      "|      2|2025-01-21 16:40:...|  NULL|    NULL|    WRITE|{mode -> Append, ...|NULL|    NULL|     NULL|          1|  Serializable|         true|{numFiles -> 3, n...|        NULL|Apache-Spark/3.5....|\n",
      "|      1|2025-01-21 16:40:...|  NULL|    NULL|   UPDATE|{predicate -> [\"(...|NULL|    NULL|     NULL|          0|  Serializable|        false|{numRemovedFiles ...|        NULL|Apache-Spark/3.5....|\n",
      "|      0|2025-01-21 16:40:...|  NULL|    NULL|    WRITE|{mode -> Overwrit...|NULL|    NULL|     NULL|       NULL|  Serializable|        false|{numFiles -> 1, n...|        NULL|Apache-Spark/3.5....|\n",
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Load the Delta table\n",
    "delta_table = DeltaTable.forPath(spark, delta_path)\n",
    "\n",
    "# Show the full history of the table\n",
    "history_df = delta_table.history()  # Returns a DataFrame of operations\n",
    "print(\"Table History:\")\n",
    "history_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd3b38c1-774a-4605-ad0a-7cf83b9a55f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+-------+-------------+--------------+--------------+----------+----------+--------------------+\n",
      "|Unique_ID|             Name|Measure|Geo_Type_Name|Geo_Place_Name|   Time_Period|Start_Date|Data_Value|Air_Quality_Category|\n",
      "+---------+-----------------+-------+-------------+--------------+--------------+----------+----------+--------------------+\n",
      "|   179772|        Emissions|Density|        UHF42|        Queens|         Other|    1/1/15|       0.3|                Good|\n",
      "|   179785|        Emissions|Density|        UHF42| Not_Specified|         Other|    1/1/15|       1.2|                Good|\n",
      "|   178540|General Pollution|  Miles|        UHF42| Not_Specified|Annual Average|   12/1/11|       8.6|                Good|\n",
      "|   178561|General Pollution|  Miles|        UHF42|        Queens|Annual Average|   12/1/11|         8|                Good|\n",
      "|   823217|General Pollution|  Miles|        UHF42|        Queens|        Summer|    6/1/22|       6.1|                Good|\n",
      "|   177910|General Pollution|  Miles|        UHF42| Not_Specified|        Summer|    6/1/12|        10|                Good|\n",
      "|   177952|General Pollution|  Miles|        UHF42| Not_Specified|        Summer|    6/1/13|       9.8|                Good|\n",
      "|   177973|General Pollution|  Miles|        UHF42|        Queens|        Summer|    6/1/13|       9.8|                Good|\n",
      "|   177931|General Pollution|  Miles|        UHF42|        Queens|        Summer|    6/1/12|       9.6|                Good|\n",
      "|   742274|General Pollution|  Miles|        UHF42|        Queens|        Summer|    6/1/21|       7.2|                Good|\n",
      "|   178582|General Pollution|  Miles|        UHF42| Not_Specified|Annual Average|   12/1/12|       8.2|                Good|\n",
      "|   178583|General Pollution|  Miles|        UHF42| Not_Specified|Annual Average|   12/1/12|       8.1|                Good|\n",
      "|   547477|General Pollution|  Miles|        UHF42|        Queens|Annual Average|    1/1/17|       6.8|                Good|\n",
      "|   547417|General Pollution|  Miles|        UHF42| Not_Specified|Annual Average|    1/1/17|       6.8|                Good|\n",
      "|   177784|General Pollution|  Miles|        UHF42| Not_Specified|        Summer|    6/1/09|      10.6|            Moderate|\n",
      "|   547414|General Pollution|  Miles|        UHF42| Not_Specified|Annual Average|    1/1/17|       7.1|                Good|\n",
      "|   130413|        Emissions|Density|        UHF42| Not_Specified|         Other|    1/1/13|       0.9|                Good|\n",
      "|   130412|        Emissions|Density|        UHF42| Not_Specified|         Other|    1/1/13|       1.7|                Good|\n",
      "|   130434|        Emissions|Density|        UHF42|        Queens|         Other|    1/1/13|         0|                Good|\n",
      "|   410847|General Pollution|  Miles|        UHF42|        Queens|        Summer|    6/1/16|       6.9|                Good|\n",
      "+---------+-----------------+-------+-------------+--------------+--------------+----------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Shape: 18016 x 9\n"
     ]
    }
   ],
   "source": [
    "# Query the Delta table as of a previous version\n",
    "df = spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(delta_path)\n",
    "\n",
    "# Display the data from a previous version\n",
    "df.show()\n",
    "\n",
    "# Get the number of rows\n",
    "row_count = df.count()\n",
    "\n",
    "# Get the number of columns\n",
    "column_count = len(df.columns)\n",
    "print(f\"Shape: {row_count} x {column_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dee4821-ef62-4d3d-b3f3-f05486c3152e",
   "metadata": {},
   "source": [
    "**Question:** \n",
    "In what scenarios would you use Delta Lake's time travel over simply maintaining snapshots of data manually? (1p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142df8a4-c48b-4da5-ab42-505713ba0c16",
   "metadata": {},
   "source": [
    "**Ans:** I would use Delta Lake's time travel in the following scenarios over manual snapshots: \n",
    "\n",
    "* Data Recovery: Quickly revert to previous versions without restoring snapshots manually.\n",
    "* Auditing and Compliance: Query historical data easily for audits, avoiding snapshot tracking.\n",
    "* Storage Efficiency: Store only data changes, unlike snapshots, which duplicate datasets.\n",
    "* Version Management: Automatically track data versions without manual effort."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc33135-a66c-4ab6-8dbb-ba4b6d10f781",
   "metadata": {},
   "source": [
    "## 5. Vacuum: Clean Up Old Files\n",
    "\n",
    "Vacuuming removes unreferenced files from the Delta table directory to optimize storage. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e274e89-d220-4d11-820d-fdd44b41041b",
   "metadata": {},
   "source": [
    "**Question:**  \n",
    "What is the default retention period for Delta table vacuuming, and why does it matter? (1p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398bdb48-e1e3-49c8-b1ef-6bd56d9a85a2",
   "metadata": {},
   "source": [
    "**Ans:** It is 7 days. \\\n",
    "And, it matters because it ensures older versions and unreferenced files are retained long enough to support operations like **time travel** , **auditing**, and **recovery** before they are permanently deleted to optimize disk space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df13bd4-2d4b-4ab9-b57e-cb12921fa1ad",
   "metadata": {},
   "source": [
    "### 6. When finished, remember to close the spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "895e36c2-9a9e-41db-a2c3-736a9ab4c32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
