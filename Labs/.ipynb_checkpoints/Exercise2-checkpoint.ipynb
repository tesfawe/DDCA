{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66f560da-b461-424f-bf1c-1dbeb09bee59",
   "metadata": {},
   "source": [
    "# **Exercise 2: Data Warehousing and Data Lakes with Spark + Hive**\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In modern data engineering, we often encounter two primary paradigms:\n",
    "1. **Data Warehouse** (schema-on-write): where data is cleansed, transformed, and loaded into structured tables before analysis.\n",
    "2. **Data Lake** (schema-on-read): where data is stored in raw format and the schema is applied when querying.\n",
    "\n",
    "This exercise showcases both approaches using **Spark** and **Hive**. You will load and query **e-commerce data** in a structured (warehouse) format, then contrast this with a more flexible (lake) approach. By the end, you should understand key **ETL/ELT** concepts, the rationale behind each paradigm, and be able to discuss the differences.\n",
    "\n",
    "**Useful links and notebooks:**\n",
    "- https://spark.apache.org/docs/latest/api/python/index.html\n",
    "- https://spark.apache.org/docs/3.5.1/sql-data-sources-hive-tables.html\n",
    "- /shared/ETL_ELT\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "\n",
    "\n",
    "This exercise is worth 18 points. To earn full points, make sure to include comments in your code explaining your approach and the reasoning behind your choices.\n",
    "\n",
    "1. **Data Warehouse Fundamentals (6p)**:  \n",
    "   - Define and create schemas using Apache Hive.\n",
    "   - Perform schema-on-write transformations and run analytical queries.\n",
    "  \n",
    "     \n",
    "---\n",
    "2. **Data Lake Fundamentals (6p)**:  \n",
    "   - Ingest raw files into Spark without predefined schema (schema-on-read).  \n",
    "   - Handle multiple file formats (CSV, JSON, TXT) in a flexible manner.\n",
    "\n",
    "---\n",
    "\n",
    "3. **Questions (6p)**:\n",
    "   - Answer three questions about the ETL and ELT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fee915-9db1-4d2f-ac0a-ad95f7995b35",
   "metadata": {},
   "source": [
    "# E-Commerce Data Schema\n",
    "\n",
    "We will be working with a couple of datasets from an e-commerce site located in the /shared folder.\n",
    "\n",
    "## 1. `customers.csv`\n",
    "- **Description**: Contains information about customers.\n",
    "- **Fields**:\n",
    "  - `customer_id` (int): Unique identifier for each customer.\n",
    "  - `name` (string): Customer's full name.\n",
    "  - `age` (int): Age of the customer.\n",
    "  - `country` (string): Country of residence.\n",
    "  - `preferred_category` (string): Preferred product category (e.g., Electronics, Books).\n",
    "  - `loyalty_score` (float): Loyalty score between 0.00 and 1.00.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. `products.csv`\n",
    "- **Description**: Contains information about products.\n",
    "- **Fields**:\n",
    "  - `product_id` (int): Unique identifier for each product.\n",
    "  - `product_name` (string): Name of the product.\n",
    "  - `category` (string): Product category (e.g., Electronics, Clothing).\n",
    "  - `price` (float): Unit price of the product.\n",
    "  - `popularity` (int): Popularity score (1–10).\n",
    "  - `region` (string): Shipping region for the product (e.g., North America, Europe).\n",
    "\n",
    "---\n",
    "\n",
    "## 3. `transactions.json`\n",
    "- **Description**: Contains information about transactions.\n",
    "- **Fields**:\n",
    "  - `transaction_id` (int): Unique identifier for each transaction.\n",
    "  - `customer_id` (int): ID referencing a row in `customers.csv`.\n",
    "  - `product_id` (int): ID referencing a row in `products.csv`.\n",
    "  - `quantity` (int): Number of items purchased in the transaction.\n",
    "  - `price` (float): Unit price of the product.\n",
    "  - `shipping_cost` (float): Shipping cost for the transaction.\n",
    "  - `tax` (float): Tax amount applied to the transaction.\n",
    "  - `total_amount` (float): Computed total cost (`quantity * price + shipping_cost + tax`).\n",
    "  - `transaction_time` (string, ISO format): Timestamp of the transaction (e.g., `YYYY-MM-DDTHH:MM:SS`).\n",
    "\n",
    "---\n",
    "\n",
    "## 4. `reviews.txt`\n",
    "- **Description**: Semi-structured text file containing product reviews.\n",
    "- **Format**: Each line follows the format: `customer_id|product_id|product_name|review_text|rating`.\n",
    "- **Fields**:\n",
    "  - `customer_id` (int): ID referencing a row in `customers.csv`.\n",
    "  - `product_id` (int): ID referencing a row in `products.csv`.\n",
    "  - `product_name` (string): Name of the reviewed product.\n",
    "  - `review_text` (string): Freeform text describing the customer’s opinion.\n",
    "  - `rating` (int): Numeric score (1–5).\n",
    "\n",
    "---\n",
    "\n",
    "## Notes\n",
    "- **Relationships**:\n",
    "  - `customer_id` links `transactions.json` and `reviews.txt` to `customers.csv`.\n",
    "  - `product_id` links `transactions.json` and `reviews.txt` to `products.csv`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90de1825-47ea-4382-990a-77912c9b888a",
   "metadata": {},
   "source": [
    "**Start by setting up a Spark session, enable Hive support so we can create databases and tables.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "039bf0ab-c9e4-4a79-af94-03ad9e9ebd4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://np-dark-orange-paronychia-7b8986bf7-2wh7s:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Exercise2</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fe22820ad50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Exercise2\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"data_warehouse\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "csv_customers_path = \"../shared/customers.csv\"\n",
    "csv_products_path = \"../shared/products.csv\"\n",
    "json_transactions_path = \"../shared/transactions.json\"\n",
    "txt_reviews_path = \"../shared/reviews.txt\"\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c84d86-43e2-4900-933c-8f4648283350",
   "metadata": {},
   "source": [
    "---\n",
    "# **1. ETL: Load data into a Data Warehouse (6p)**\n",
    "\n",
    "## Instructions\n",
    "1. Define the following tables:\n",
    "   - **`customers`**\n",
    "   - **`products`**\n",
    "   - **`transactions`**\n",
    "   - **`reviews`**\n",
    "2. Use **Parquet** format for optimized storage and query performance.\n",
    "3. Write `CREATE TABLE` statements in Hive to define the schema.\n",
    "4. **Optional**: Consider partitioning tables if you think it's reasonable, and explain the reasoning behind your decision.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73298bee-19ef-4feb-9df7-217633d98cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Databases in Spark:\n",
      "+-------------+\n",
      "|    namespace|\n",
      "+-------------+\n",
      "|      default|\n",
      "|exercise_2_db|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS exercise_2_db\")\n",
    "spark.sql(\"USE exercise_2_db\")\n",
    "\n",
    "print(\"Databases in Spark:\")\n",
    "spark.sql(\"SHOW DATABASES\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cc22ae-6f52-4825-b2bf-4b33ff7e3801",
   "metadata": {},
   "source": [
    "**Tables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2981f30f-52f3-4dda-9e79-04065e8b5a8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the tables\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS exercise_2_db.customers (\n",
    "    customer_id INT,\n",
    "    name STRING,\n",
    "    age INT,\n",
    "    country STRING,\n",
    "    preferred_category STRING,\n",
    "    loyalty_score FLOAT\n",
    ")\n",
    "USING PARQUET;\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS exercise_2_db.products (\n",
    "    product_id INT,\n",
    "    product_name STRING,\n",
    "    category STRING,\n",
    "    price FLOAT,\n",
    "    popularity INT,\n",
    "    region STRING\n",
    ")\n",
    "USING PARQUET;\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS exercise_2_db.transactions (\n",
    "    transaction_id INT,\n",
    "    customer_id INT,\n",
    "    product_id INT,\n",
    "    quantity INT,\n",
    "    price FLOAT,\n",
    "    shipping_cost FLOAT,\n",
    "    tax FLOAT,\n",
    "    total_amount FLOAT,\n",
    "    transaction_time TIMESTAMP\n",
    ")\n",
    "USING PARQUET;\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS exercise_2_db.reviews (\n",
    "    customer_id INT,\n",
    "    product_id INT,\n",
    "    review_text STRING,\n",
    "    rating INT\n",
    ")\n",
    "USING PARQUET;\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4f6e35-888c-4cbe-aa29-6a292cc2a56a",
   "metadata": {},
   "source": [
    "### **ETL Process**\n",
    "\n",
    "Now that we have defined the tables we can extract raw data, clean it, and load it into the predefined tables.\n",
    "\n",
    "### Instructions\n",
    "1. Read raw data from the provided files located in the shared folder (`customers.csv`, `products.csv`, `transactions.json`, `reviews.txt`).\n",
    "2. Apply transformations:\n",
    "   - Cast columns to the correct data types.\n",
    "   - Handle missing or invalid data (e.g., filter out rows with null IDs, if such rows exist)\n",
    "   - Only insert the columns you find necessary.\n",
    "3. Use spark.sql or DataFrame APIs to insert the cleaned data into the warehouse tables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c80ad7c-87ad-4696-ab78-145c119fcf3c",
   "metadata": {},
   "source": [
    "**Customers ETL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13cf6f78-5348-4e47-bb53-e6d7abc243cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Source Data (Customers) ===\n",
      "+-----------+-------------+---+--------------+------------------+-------------+\n",
      "|customer_id|name         |age|country       |preferred_category|loyalty_score|\n",
      "+-----------+-------------+---+--------------+------------------+-------------+\n",
      "|1          |Cindy Simpson|60 |United Kingdom|Clothing          |0.15         |\n",
      "|2          |Eric White   |41 |United Kingdom|Clothing          |0.22         |\n",
      "|3          |Linda Todd   |54 |United Kingdom|Home              |0.5          |\n",
      "|4          |Shannon Woods|52 |Canada        |Sports            |0.71         |\n",
      "|5          |Michael Brown|48 |France        |Clothing          |0.36         |\n",
      "+-----------+-------------+---+--------------+------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, col\n",
    "\n",
    "df_source = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .csv(csv_customers_path)\n",
    "\n",
    "print(\"=== Source Data (Customers) ===\")\n",
    "df_source.show(5, truncate=False)\n",
    "\n",
    "# Transform: Remove null customer_id and cast data types \n",
    "df_cleaned = df_source\\\n",
    "    .filter(col(\"customer_id\").isNotNull()) \\\n",
    "    .withColumn(\"age\", col(\"age\").cast(\"int\")) \\\n",
    "    .withColumn(\"loyalty_score\", col(\"loyalty_score\").cast(\"float\"))\n",
    "\n",
    "# Load into table\n",
    "df_cleaned.write.mode(\"overwrite\").format(\"parquet\").saveAsTable(\"exercise_2_db.customers\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3e94f3-4560-4a85-80ce-ac7de228cd54",
   "metadata": {},
   "source": [
    "**Products ETL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac4b8b83-1ed6-4603-9ebc-acc24f2a212e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Source Data (Products) ===\n",
      "+----------+--------------+-----------+------+----------+-------------+\n",
      "|product_id|product_name  |category   |price |popularity|region       |\n",
      "+----------+--------------+-----------+------+----------+-------------+\n",
      "|1         |Raincoat      |Clothing   |43.99 |10        |North America|\n",
      "|2         |Sneakers      |Clothing   |25.99 |6         |Europe       |\n",
      "|3         |Self-Help Book|Books      |48.99 |6         |Europe       |\n",
      "|4         |Action Camera |Electronics|680.99|7         |North America|\n",
      "|5         |4K Monitor    |Electronics|824.99|5         |North America|\n",
      "+----------+--------------+-----------+------+----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_source = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .csv(csv_products_path)\n",
    "\n",
    "print(\"=== Source Data (Products) ===\")\n",
    "df_source.show(5, truncate=False)\n",
    "\n",
    "# Transform: Remove null product_id and cast data types \n",
    "df_cleaned = df_source \\\n",
    "    .filter(col(\"product_id\").isNotNull()) \\\n",
    "    .withColumn(\"price\", col(\"price\").cast(\"float\")) \\\n",
    "    .withColumn(\"popularity\", col(\"popularity\").cast(\"int\")) \n",
    "# Load into table\n",
    "df_cleaned.write.mode(\"overwrite\").format(\"parquet\").saveAsTable(\"exercise_2_db.products\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988d6eee-a477-4811-afb1-badd13cd9538",
   "metadata": {},
   "source": [
    "**Transactions ETL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed265397-3423-4122-b1a8-11e7e0240148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Source Data (Transactions) ===\n",
      "+-----------+------+----------+--------+-------------+----+------------+--------------+--------------------------+\n",
      "|customer_id|price |product_id|quantity|shipping_cost|tax |total_amount|transaction_id|transaction_time          |\n",
      "+-----------+------+----------+--------+-------------+----+------------+--------------+--------------------------+\n",
      "|32         |77.99 |22        |3       |11.03        |23.4|268.4       |1             |2024-11-09T09:48:12.057267|\n",
      "|40         |384.99|40        |1       |13.44        |38.5|436.93      |2             |2024-08-06T08:26:45.609302|\n",
      "|10         |174.99|18        |1       |19.44        |17.5|211.93      |3             |2024-02-26T12:33:55.105117|\n",
      "|91         |19.99 |20        |5       |11.71        |9.99|121.65      |4             |2024-08-15T16:23:58.540147|\n",
      "|86         |161.99|42        |2       |15.7         |32.4|372.08      |5             |2024-03-03T21:44:01.045684|\n",
      "+-----------+------+----------+--------+-------------+----+------------+--------------+--------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_source = spark.read \\\n",
    "    .option(\"multiline\", True) \\\n",
    "    .json(json_transactions_path)\n",
    "\n",
    "print(\"=== Source Data (Transactions) ===\")\n",
    "df_source.show(5, truncate=False)\n",
    "\n",
    "# Transform: Convert transaction_time to TIMESTAMP, remove invalid rows and cast data types \n",
    "df_cleaned = df_source \\\n",
    "    .filter(\n",
    "        col(\"transaction_id\").isNotNull() &\n",
    "        col(\"customer_id\").isNotNull() &\n",
    "        col(\"product_id\").isNotNull()\n",
    "    ) \\\n",
    "    .withColumn(\"transaction_id\", col(\"transaction_id\").cast(\"int\")) \\\n",
    "    .withColumn(\"quantity\", col(\"quantity\").cast(\"int\")) \\\n",
    "    .withColumn(\"price\", col(\"price\").cast(\"float\")) \\\n",
    "    .withColumn(\"shipping_cost\", col(\"shipping_cost\").cast(\"float\")) \\\n",
    "    .withColumn(\"tax\", col(\"tax\").cast(\"float\")) \\\n",
    "    .withColumn(\"total_amount\", col(\"total_amount\").cast(\"float\")) \\\n",
    "    .withColumn(\"transaction_time\", col(\"transaction_time\").cast(\"timestamp\"))\n",
    "\n",
    "# Load into table\n",
    "df_cleaned.write.mode(\"overwrite\").format(\"parquet\").saveAsTable(\"exercise_2_db.transactions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7980db5-014a-453e-a48e-aaa130660afc",
   "metadata": {},
   "source": [
    "**Reviews ETL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ff2ac0d-fff9-4c9f-a0a1-a6c3d76c6d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Source Data (Reviews) ===\n",
      "+-----------+----------+------------------+----------------------------------------------------------------------------------------------------+------+\n",
      "|customer_id|product_id|product_name      |review_text                                                                                         |rating|\n",
      "+-----------+----------+------------------+----------------------------------------------------------------------------------------------------+------+\n",
      "|5          |16        |Running Shoes     |Absolutely worth the price! The quality is unmatched. Amazing product, highly recommend!            |5     |\n",
      "|64         |46        |Exercise Bike     |Absolutely worth the price! The quality is unmatched. Amazing product, highly recommend!            |5     |\n",
      "|65         |14        |Gaming Console    |Fantastic build quality. You get what you pay for! Audio quality is clear and immersive.            |4     |\n",
      "|67         |38        |Historical Fiction|A premium product that delivers premium results. A must-read for fans of the genre.                 |5     |\n",
      "|64         |4         |Action Camera     |Absolutely worth the price! The quality is unmatched. The battery life is phenomenal, lasts all day!|5     |\n",
      "+-----------+----------+------------------+----------------------------------------------------------------------------------------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_source = spark.read.text(txt_reviews_path)\n",
    "\n",
    "print(\"=== Source Data (Reviews) ===\")\n",
    "\n",
    "# Split the column into separate fields using \"|\"\n",
    "df_split = df_source.withColumn(\"customer_id\", split(col(\"value\"), \"\\\\|\")[0].cast(\"int\")) \\\n",
    "    .withColumn(\"product_id\", split(col(\"value\"), \"\\\\|\")[1].cast(\"int\")) \\\n",
    "    .withColumn(\"product_name\", split(col(\"value\"), \"\\\\|\")[2].cast(\"string\")) \\\n",
    "    .withColumn(\"review_text\", split(col(\"value\"), \"\\\\|\")[3].cast(\"string\")) \\\n",
    "    .withColumn(\"rating\", split(col(\"value\"), \"\\\\|\")[4].cast(\"int\")) \\\n",
    "    .drop(\"value\") \n",
    "\n",
    "df_split.show(5, truncate=False)\n",
    "\n",
    "# Transform: Drop product_name for normalization purposes, remove invalid rows\n",
    "df_cleaned = df_split.drop(\"product_name\") \\\n",
    "    .filter(df_split.customer_id.isNotNull() & df_split.product_id.isNotNull())\n",
    "\n",
    "# Load into table\n",
    "df_cleaned.write.mode(\"overwrite\").format(\"parquet\").saveAsTable(\"exercise_2_db.reviews\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c560b09-da81-4679-9256-04c0be4510a2",
   "metadata": {},
   "source": [
    "## Analyze the Data\n",
    "\n",
    "## Objective\n",
    "Run SQL queries to analyze the transformed data.\n",
    "\n",
    "### Example Queries to Run\n",
    "\n",
    "1. **Total Revenue and Transactions per Product Category**  \n",
    "  \n",
    "\n",
    "2. **Identify the 5 Least Sold Products**  \n",
    "  \n",
    "\n",
    "3. **Identify the Top 5 Spending Customers**  \n",
    "\n",
    "\n",
    "**You are encouraged to run these queries, but feel free to explore the data and create your own queries if you believe they provide better insights or are more relevant for analysis.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b9e15d-c828-4fef-b5a1-b9c8f0d541bb",
   "metadata": {},
   "source": [
    "1. **Total Revenue and Transactions per Product Category**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03d89538-e974-4ce0-b4d3-6fce97626f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+-------------+\n",
      "|   category|total_transactions|total_revenue|\n",
      "+-----------+------------------+-------------+\n",
      "|Electronics|               189|    154211.24|\n",
      "|   Clothing|               300|     65483.09|\n",
      "|     Sports|               171|     61551.06|\n",
      "|       Home|               111|     50321.74|\n",
      "|      Books|               229|     25490.83|\n",
      "+-----------+------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    products.category, \n",
    "    COUNT(transactions.transaction_id) AS total_transactions, \n",
    "    ROUND(SUM(transactions.total_amount), 2) AS total_revenue\n",
    "FROM exercise_2_db.transactions \n",
    "JOIN exercise_2_db.products ON transactions.product_id = products.product_id\n",
    "GROUP BY products.category\n",
    "ORDER BY total_revenue DESC\n",
    "\"\"\"\n",
    "\n",
    "df_result = spark.sql(query)\n",
    "df_result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cd0edc-5844-4647-af3e-7b188ca8bb8e",
   "metadata": {},
   "source": [
    "2. **5 Least Sold Products** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdd36323-e9e3-4660-9c96-16d8a72c7d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|        product_name|total_quantity_sold|\n",
      "+--------------------+-------------------+\n",
      "|        Air Purifier|                  5|\n",
      "|          Wall Clock|                  5|\n",
      "|        Coffee Maker|                  5|\n",
      "|       Action Camera|                  7|\n",
      "|Noise-Canceling H...|                  9|\n",
      "+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    products.product_name, \n",
    "    SUM(transactions.quantity) AS total_quantity_sold\n",
    "FROM exercise_2_db.transactions \n",
    "JOIN exercise_2_db.products ON transactions.product_id = products.product_id\n",
    "GROUP BY products.product_name\n",
    "ORDER BY total_quantity_sold ASC\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "\n",
    "df_result = spark.sql(query)\n",
    "df_result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca725ca-7e57-4829-9050-a86ac37c097d",
   "metadata": {},
   "source": [
    "3. **Top 5 Spending Customers** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f474ae9-6f83-47df-b178-972b239ba504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------+\n",
      "|   customer_name|total_spent|\n",
      "+----------------+-----------+\n",
      "|     Nancy Jones|   15662.42|\n",
      "|     Cesar Davis|   14997.11|\n",
      "|Valerie Mitchell|   14160.32|\n",
      "|  Anthony Pruitt|   12767.85|\n",
      "|  Nicholas Davis|   11635.65|\n",
      "+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    customers.name AS customer_name, \n",
    "    ROUND(SUM(transactions.total_amount), 2)  AS total_spent\n",
    "FROM exercise_2_db.transactions \n",
    "JOIN exercise_2_db.customers ON transactions.customer_id = customers.customer_id\n",
    "GROUP BY customers.name\n",
    "ORDER BY total_spent DESC\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "\n",
    "df_result = spark.sql(query)\n",
    "df_result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7193d66-78d0-4989-b8c0-5bb2c4407a70",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. **ELT: Load Raw Data into a Data Lake (6p)**\n",
    "\n",
    "## Objective\n",
    "Copy the raw data files into a `data_lake/` directory and transform the data on read.\n",
    "\n",
    "### Instructions\n",
    "1. Copy or use shell commands or scripts to move the files into a `data_lake/` directory in your `my-work` folder.\n",
    "2. Do not modify the files; load them “as is” to retain their raw state.\n",
    "\n",
    "Now the `data_lake/` folder contains all raw files, unmodified:\n",
    "\n",
    "```plaintext\n",
    "data_lake/\n",
    "├── customers.csv\n",
    "├── products.csv\n",
    "├── transactions.json\n",
    "└── reviews.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf88f40-eadf-4c2b-bdda-ff198508b350",
   "metadata": {},
   "source": [
    "# **Transform and Analyze**\n",
    "\n",
    "### Instructions\n",
    "1. Read the raw files from the `data_lake/` directory using Spark.\n",
    "2. Clean and transform the data on read.\n",
    "3. Register the transformed DataFrames as temporary views.\n",
    "4. Run the same queries as in the warehouse approach:\n",
    "   \n",
    "- Total Revenue and Transactions per Product Category\n",
    "  \n",
    "- Identify the 5 Least Sold Products\n",
    "  \n",
    "- Identify the Top 5 Customers by Spending\n",
    "\n",
    "\n",
    "**You are encouraged to run these queries, but feel free to explore the data and create your own queries if you believe they provide better insights or are more relevant for analysis.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df98d7b-3aad-4d89-b14e-258f28bf3277",
   "metadata": {},
   "source": [
    "#### Copy the raw data files into a data_lake/ directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b469b6b9-24da-43a5-938d-62eff558a404",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data_lake\n",
    "!cp ../shared/customers.csv data_lake/\n",
    "!cp ../shared/products.csv data_lake/\n",
    "!cp ../shared/transactions.json data_lake/\n",
    "!cp ../shared/reviews.txt data_lake/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9b83dc-c289-476f-be2f-293086ba65cd",
   "metadata": {},
   "source": [
    "\n",
    "1. **Read the raw files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdc649ff-e031-409a-96d5-fa2e93abde83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customers = spark.read.csv(\"data_lake/customers.csv\", header=True)\n",
    "df_products = spark.read.csv(\"data_lake/products.csv\", header=True)\n",
    "df_transactions = spark.read.json(\"data_lake/transactions.json\")\n",
    "df_reviews = spark.read.text(\"data_lake/reviews.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0a5ba3-29d5-46a4-8a38-5bc0f00b1177",
   "metadata": {},
   "source": [
    "2. **Clean and transform the data on read.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43d34101-3d34-45ab-a8ce-5ce4973f6efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_customers = df_customers.select(\n",
    "    col(\"customer_id\").cast(\"int\"),\n",
    "    col(\"name\"),\n",
    "    col(\"age\").cast(\"int\"),\n",
    "    col(\"country\"),\n",
    "    col(\"preferred_category\"),\n",
    "    col(\"loyalty_score\").cast(\"float\")\n",
    ")\n",
    "\n",
    "cleaned_customers = transformed_customers.filter(col(\"customer_id\").isNotNull())\n",
    "\n",
    "transformed_products = df_products.select(\n",
    "    col(\"product_id\").cast(\"int\"),\n",
    "    col(\"product_name\"),\n",
    "    col(\"category\"),\n",
    "    col(\"price\").cast(\"float\"),\n",
    "    col(\"popularity\").cast(\"int\"),\n",
    "    col(\"region\")\n",
    ")\n",
    "\n",
    "cleaned_products = transformed_products.filter(col(\"product_id\").isNotNull())\n",
    "\n",
    "transformed_transactions = df_transactions.select(\n",
    "    col(\"transaction_id\").cast(\"int\"),\n",
    "    col(\"customer_id\").cast(\"int\"),\n",
    "    col(\"product_id\").cast(\"int\"),\n",
    "    col(\"quantity\").cast(\"int\"),\n",
    "    col(\"price\").cast(\"float\"),\n",
    "    col(\"shipping_cost\").cast(\"float\"),\n",
    "    col(\"tax\").cast(\"float\"),\n",
    "    col(\"total_amount\").cast(\"float\"),\n",
    "    col(\"transaction_time\").cast(\"timestamp\")\n",
    ")\n",
    "\n",
    "cleaned_transactions = transformed_transactions \\\n",
    "    .filter(\n",
    "        col(\"transaction_id\").isNotNull() &\n",
    "        col(\"customer_id\").isNotNull() &\n",
    "        col(\"product_id\").isNotNull()\n",
    "    ) \\\n",
    "    .withColumn(\"transaction_time\", col(\"transaction_time\").cast(\"timestamp\"))\n",
    "\n",
    "transformed_reviews = df_reviews.select(\n",
    "    split(col(\"value\"), \"\\|\").getItem(0).cast(\"int\").alias(\"customer_id\"),\n",
    "    split(col(\"value\"), \"\\|\").getItem(1).cast(\"int\").alias(\"product_id\"),\n",
    "    split(col(\"value\"), \"\\|\").getItem(2).alias(\"product_name\"),\n",
    "    split(col(\"value\"), \"\\|\").getItem(3).alias(\"review_text\"),\n",
    "    split(col(\"value\"), \"\\|\").getItem(4).cast(\"int\").alias(\"rating\")\n",
    ").drop(\"value\")\n",
    "\n",
    "cleaned_reviews = transformed_reviews.drop(\"product_name\") \\\n",
    "    .filter(transformed_reviews.customer_id.isNotNull() & transformed_reviews.product_id.isNotNull())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282d153b-f976-4115-9d4f-18a7c6aca2ad",
   "metadata": {},
   "source": [
    "3. **Register the transformed DataFrames as temporary views.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bff71fa2-eeaf-474b-8d90-efdae862b58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_customers.createOrReplaceTempView(\"customers_view\")\n",
    "cleaned_products.createOrReplaceTempView(\"products_view\")\n",
    "cleaned_transactions.createOrReplaceTempView(\"transactions_view\")\n",
    "cleaned_reviews.createOrReplaceTempView(\"reviews_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8de13e-b5d6-4c6d-a5c6-bb9fef8330b4",
   "metadata": {},
   "source": [
    "### Analyze the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0602e6c3-fcb7-4484-8050-545f33c685f3",
   "metadata": {},
   "source": [
    "4. **Run the same queries as in the warehouse approach:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5985a4-5922-4dd7-97cb-24df40ca6433",
   "metadata": {},
   "source": [
    "4.1. **Total Revenue and Transactions per Product Category**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2cf349bf-e041-440d-b70e-c7b8be352295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+-------------+\n",
      "|category   |total_transactions|total_revenue|\n",
      "+-----------+------------------+-------------+\n",
      "|Electronics|189               |154211.24    |\n",
      "|Clothing   |300               |65483.09     |\n",
      "|Sports     |171               |61551.06     |\n",
      "|Home       |111               |50321.74     |\n",
      "|Books      |229               |25490.83     |\n",
      "+-----------+------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        p.category, \n",
    "        COUNT(t.transaction_id) AS total_transactions,\n",
    "        ROUND(SUM(t.total_amount), 2) AS total_revenue\n",
    "    FROM transactions_view AS t\n",
    "    JOIN products_view AS p ON t.product_id = p.product_id\n",
    "    GROUP BY p.category\n",
    "    ORDER BY total_revenue DESC\n",
    "\"\"\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb96ff0-dbf0-472c-953d-9c52a2e32dd7",
   "metadata": {},
   "source": [
    "4.2. **5 Least Sold Products**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59193bf6-0811-4ff6-8a80-663ad8d2bcd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+----------+\n",
      "|product_name              |total_sold|\n",
      "+--------------------------+----------+\n",
      "|Air Purifier              |5         |\n",
      "|Wall Clock                |5         |\n",
      "|Coffee Maker              |5         |\n",
      "|Action Camera             |7         |\n",
      "|Noise-Canceling Headphones|9         |\n",
      "+--------------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        p.product_name, \n",
    "        SUM(t.quantity) AS total_sold\n",
    "    FROM transactions_view AS t\n",
    "    JOIN products_view As p ON t.product_id = p.product_id\n",
    "    GROUP BY p.product_name\n",
    "    ORDER BY total_sold ASC\n",
    "    LIMIT 5\n",
    "\"\"\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0611dcef-0ee7-41dc-b67d-6f2d11b379fe",
   "metadata": {},
   "source": [
    "4.3. **Top 5 Customers by Spending**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8bd8a0ef-8a39-4e98-96a8-a4ba38271136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------+\n",
      "|customer_name   |total_spent|\n",
      "+----------------+-----------+\n",
      "|Nancy Jones     |15662.42   |\n",
      "|Cesar Davis     |14997.11   |\n",
      "|Valerie Mitchell|14160.32   |\n",
      "|Anthony Pruitt  |12767.85   |\n",
      "|Nicholas Davis  |11635.65   |\n",
      "+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        c.name AS customer_name, \n",
    "        ROUND(SUM(t.total_amount), 2) AS total_spent\n",
    "    FROM transactions_view AS t\n",
    "    JOIN customers_view AS c ON t.customer_id = c.customer_id\n",
    "    GROUP BY c.name\n",
    "    ORDER BY total_spent DESC\n",
    "    LIMIT 5\n",
    "\"\"\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a644e99d-5d1f-4e67-a338-a4a94a417df0",
   "metadata": {},
   "source": [
    "---\n",
    "### **Questions (6p)**\n",
    "\n",
    "Reflect on the following questions and provide thoughtful answers. Focus on your reasoning, insights, and key takeaways from the exercise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd2c4a4-67e4-431d-bf5f-5f61fe1697cb",
   "metadata": {},
   "source": [
    "**1. What were the key differences in how data was handled and queried in the warehouse (ETL) versus the lake (ELT)? Which approach felt more adaptable to changes in data structure or format, and why?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6ca7b5-0d23-4441-b3ee-39c695b9ab3f",
   "metadata": {},
   "source": [
    "The key difference between ETL and ELT lies in when and where data transformation happens:\n",
    "\n",
    "**ETL (Warehouse):** Data is transformed before loading into the warehouse, ensuring structured, clean, and optimized data for queries. ETL queries more efficient but can be rigid when handling evolving schemas. \n",
    "\n",
    "**ELT (Lake):** Raw data is loaded first as-is, and the transformation happens later as needed. ELT is flexible for schema changes and accommodates semi-structured data, but queries were slower due to on-the-fly transformations.\n",
    "\n",
    "For adaptability, **ELT** is more flexible, especially when handling different data formats (Example: CSV, JSON and TXT files in our case), since schema changes do not require reloading the entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906df7f7-59d4-4778-93e8-2083f454e063",
   "metadata": {},
   "source": [
    "**2. What challenges did you encounter when transforming and querying the data in each approach? How did these challenges help you better understand the trade-offs of schema-on-write vs. schema-on-read?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8309283d-8265-428c-86a6-216cdcf5709b",
   "metadata": {},
   "source": [
    "**ETL challenges:** Defining a strict schema and transforming raw data upfront before loading it into the storage was time-consuming. This can be seen as a drawback when fast data ingestion is a priority, especially with increasingly varied unstructured data.. However, queries were straight-forward and efficient. \n",
    "\n",
    "**ELT challenges:** We needed to transform the raw data and define the schema at runtime, which made the queries slower. Schema-on-read made querying more complex. However, this approach can be more suitable when the speed of data ingestion is a priority. \n",
    "\n",
    "These challenges helped us to understand that schema-on-write ensures cleaner, optimized queries but lacks flexibility, while schema-on-read adapts better to diverse data but requires more processing at query time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d204c1d-4633-4059-84d2-0f63ad6a85b8",
   "metadata": {},
   "source": [
    "**3. What factors would you consider when deciding between a warehouse, a lake, or a hybrid approach for a real-world data solution?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9705d8-9e9f-4bc5-a679-aa5f336ce0ec",
   "metadata": {},
   "source": [
    "We would consider several factors:\n",
    "\n",
    "1. **Data Type & Structure:** If dealing with structured data, a warehouse is better choice. For unstructured or semi-structured data, a lake is better.\n",
    "2. **Query Performance Needs:** Warehouses are optimized for fast analytics, while lakes may require extra processing.\n",
    "3. **Scalability & Cost:** Data lakes offer low-cost storage and can handle massive datasets. \n",
    "\n",
    "So, \\\n",
    "**Data Warehouse** → If real-time analytics and business intelligence are needed \\\n",
    "**Data Lakes** → If flexibility and machine learning applications are the focus \\\n",
    "**Hybrid Approach** → If both structured reporting and unstructured data analysis are required"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
